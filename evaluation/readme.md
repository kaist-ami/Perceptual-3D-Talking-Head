# Evaluation

This directory provides three evaluation pipelines:

1. **Mean Temporal Misalignment (MTM)** â€“ temporal discrepancy between speech and corresponding lip movements.
2. **Perceptual Lip Readability Score (PLRS)** -  perceptual alignment between lip movements and speech.
3. **Speech-Lip Intensity Correlation Coefficient (SLCC)** â€“ expressiveness correlation between lip movements and speech

---

## ðŸ•’ Mean Temporal Misalignment (MTM)

This script computes the **Mean Temporal Misalignment** between ground-truth and predicted vertex sequences.  
Example `.npy` files (ground-truth / FaceFormer predictions) are included. 

Note that the metric also supports **one-to-many comparisons**â€”e.g. a single ground-truth sequence vs. multiple predictions conditioned on speaker identity.

### Run

```bash
cd evaluation
python evaluate_MTM.py
```

### Output

* A **CSV** file per clip containing  
  * **Mean Î”t (frames)** â€“ average temporal offset  
  * **# matching points** â€“ matched vertex pairs  
  * **Î”t per point** â€“ frame-wise misalignment  
* A **PNG** visualization for each clip.

### Convert to ms

If your dataset is **25 FPS**:

```
Î”t (ms) = Î”t (frames) Ã— 40 ms
```

---

## ðŸ“ˆ Speech-Lip Intensity Correlation Coefficient (SLCC)

This pipeline correlates **speech intensity** (audio RMS) with **lip-motion intensity** (vertex displacement) to quantify expressiveness.

### â‘   Download MEAD

1. Grab MEAD from **[Google Drive](https://drive.google.com/drive/folders/1GwXP-KpWOxOenOxITTsURJZQ_1pkd4-j)**.  
2. Place it here:

```
evaluation/MEAD
```

3. Directory must look like:

```
evaluation/
â””â”€â”€ MEAD
    â”œâ”€â”€ M030
    â”‚   â”œâ”€â”€ images
    â”‚   â””â”€â”€ video
    â”‚       â”œâ”€â”€ front
    â”‚       â”œâ”€â”€ down
    â”‚       â”œâ”€â”€ left_30
    â”‚       â”œâ”€â”€ left_60
    â”‚       â”œâ”€â”€ right_30
    â”‚       â”œâ”€â”€ right_60
    â”‚       â””â”€â”€ ...
    â”‚           â””â”€â”€ angry
    â”‚               â”œâ”€â”€ level_1
    â”‚               â”‚   â”œâ”€â”€ 001.mp4
    â”‚               â”‚   â””â”€â”€ ...
    â”‚               â”œâ”€â”€ level_2
    â”‚               â””â”€â”€ level_3
    â”œâ”€â”€ M031
    â””â”€â”€ ...
```

### â‘¡  Extract Speech Intensity (SI)

```bash
cd evaluation
python extract_rms.py
```

This writes an **RMS CSV** for every video clip.

### â‘¢  Extract Lip Intensity (LI)

1. Put your predicted vertex files in:

```
evaluation/data_SLCC/
```

2. File-name format **(required)**:

```
{id}_{emotion}_{level}_{clip}_condition_{condition_id}.npy
```

*Example*

```
M035_angry_level_2_001_condition_FaceTalk_170725_00137_TA.npy
```

3. Run:

```bash
python extract_lip_intensity.py
```

This produces a **lip-displacement CSV** for each clip.

### â‘£  SLCC Evaluation

```bash
python extract_lip_intensity.py   # second pass computes SLCC
```

#### Results

* **Overall SLCC**  
* **SLCC per expression level** (`level_1`, `level_2`, `level_3`)

Plots and summary tables are saved to `SLCC_results/`.

---
