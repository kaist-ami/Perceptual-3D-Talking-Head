# Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics 
<h3>CVPR 2025 <mark>Highlight</mark></h3>


### [Project Page](https://perceptual-3d-talking-head.github.io/) | [Paper](https://arxiv.org/pdf/2503.20308)

![Image](https://github.com/user-attachments/assets/90a114a5-5bc0-49dc-bb3b-b069784e4328)

<div align="center">
We define three criteria to assess perceptual alignment between speech and lip movements of 3D talking heads: <br>
Temporal Synchronization, Lip Readability, and Expressiveness.
</div>
<br>

This repository includes **speech-mesh synchronized representation** and their usage as **a perceptual loss**. 
We also provide **the evaluation codes for three metrics**â€”MTM, PLRS, and SLCCâ€”to assess how well the generated 3D talking heads align with the three criteria.

# Evaluation

This directory provides three evaluation pipelines:

1. **Mean Temporal Misalignment (MTM)** â€“ temporal discrepancy between speech and corresponding lip movements.
2. **Perceptual Lip Readability Score (PLRS)** -  perceptual alignment between lip movements and speech.
3. **Speech-Lip Intensity Correlation Coefficient (SLCC)** â€“ expressiveness correlation between lip movements and speech

---

## ðŸ•’ Mean Temporal Misalignment (MTM)

This script computes the **Mean Temporal Misalignment** between ground-truth and predicted vertex sequences.  
Example `.npy` files (ground-truth / FaceFormer predictions) are included. 

Note that the metric also supports **one-to-many comparisons**â€”e.g. a single ground-truth sequence vs. multiple predictions conditioned on speaker identity.

### Run

```bash
cd evaluation
python evaluate_MTM.py
```

### Output

* A **CSV** file per clip containing  
  * **Mean Î”t (frames)** â€“ average temporal offset  
  * **# matching points** â€“ matched vertex pairs  
  * **Î”t per point** â€“ frame-wise misalignment  
* A **PNG** visualization for each clip.

### Convert to ms

If your dataset is **25 FPS**:

```
Î”t (ms) = Î”t (frames) Ã— 40 ms
```

---

## ðŸ“ˆ Speech-Lip Intensity Correlation Coefficient (SLCC)

This pipeline correlates **speech intensity** (audio RMS) with **lip-motion intensity** (vertex displacement) to quantify expressiveness.

### â‘   Download MEAD

1. Grab MEAD from **[Google Drive](https://drive.google.com/drive/folders/1GwXP-KpWOxOenOxITTsURJZQ_1pkd4-j)**.  
2. Place it here:

```
evaluation/MEAD
```

3. Directory must look like:

```
evaluation/
â””â”€â”€ MEAD
    â”œâ”€â”€ M030
    â”‚   â”œâ”€â”€ images
    â”‚   â””â”€â”€ video
    â”‚       â”œâ”€â”€ front
    â”‚       â”œâ”€â”€ down
    â”‚       â”œâ”€â”€ left_30
    â”‚       â”œâ”€â”€ left_60
    â”‚       â”œâ”€â”€ right_30
    â”‚       â”œâ”€â”€ right_60
    â”‚       â””â”€â”€ ...
    â”‚           â””â”€â”€ angry
    â”‚               â”œâ”€â”€ level_1
    â”‚               â”‚   â”œâ”€â”€ 001.mp4
    â”‚               â”‚   â””â”€â”€ ...
    â”‚               â”œâ”€â”€ level_2
    â”‚               â””â”€â”€ level_3
    â”œâ”€â”€ M031
    â””â”€â”€ ...
```

### â‘¡  Extract Speech Intensity (SI)

```bash
cd evaluation
python extract_rms.py
```

This writes an **RMS CSV** for every video clip.

### â‘¢  Extract Lip Intensity (LI)

1. Put your predicted vertex files in:

```
evaluation/data_SLCC/
```

2. File-name format **(required)**:

```
{id}_{emotion}_{level}_{clip}_condition_{condition_id}.npy
```

*Example*

```
M035_angry_level_2_001_condition_FaceTalk_170725_00137_TA.npy
```

3. Run:

```bash
python extract_lip_intensity.py
```

This produces a **lip-displacement CSV** for each clip.

### â‘£  SLCC Evaluation

```bash
python extract_lip_intensity.py   # second pass computes SLCC
```

#### Results

* **Overall SLCC**  
* **SLCC per expression level** (`level_1`, `level_2`, `level_3`)

Plots and summary tables are saved to `SLCC_results/`.

---


## ðŸ“š Citation
If you found this code useful, please consider citing our paper.

```
@article{chae2025perceptually,
  title={Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics},
  author={Chae-Yeon, Lee and Hyun-Bin, Oh and EunGi, Han and Sung-Bin, Kim and Nam, Suekyeong and Oh, Tae-Hyun},
  journal={arXiv preprint arXiv:2503.20308},
  year={2025}
}
```
